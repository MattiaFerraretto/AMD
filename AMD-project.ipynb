{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhaHfQbrnBhz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from scipy.sparse import coo_array, csr_array, csc_array, lil_array\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import pandas as pd\n",
        "from tempfile import mkdtemp\n",
        "import time\n",
        "import multiprocessing\n",
        "import os\n",
        "import pickle\n",
        "import cupy as cp\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsPPXaRmCugR",
        "outputId": "13421ea7-2592-4120-8eb8-f6ad90234d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading yelp_academic_dataset_review.json.zip to /content\n",
            "100% 2.06G/2.07G [00:20<00:00, 126MB/s]\n",
            "100% 2.07G/2.07G [00:20<00:00, 110MB/s]\n",
            "Downloading yelp_academic_dataset_business.json.zip to /content\n",
            " 43% 9.00M/20.8M [00:00<00:00, 38.3MB/s]\n",
            "100% 20.8M/20.8M [00:00<00:00, 74.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "os.environ['KAGGLE_USERNAME'] = \"XXXXXXXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXXXXXXX\"\n",
        "!kaggle datasets download yelp-dataset/yelp-dataset -f  yelp_academic_dataset_review.json\n",
        "!kaggle datasets download yelp-dataset/yelp-dataset -f  yelp_academic_dataset_business.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iy-kUGGqjOsD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "BMap class is a bidirectional map used to mantain the relashonship between alphanumeric ids and numeric ids\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "map : list of tuples [(t, i)] or a set of tokens\n",
        "\n",
        "'''\n",
        "def t2if(): return -1\n",
        "def i2tf(): return 'NULL'\n",
        "\n",
        "class BMap:\n",
        "\n",
        "  def __init__(self, map=None):\n",
        "\n",
        "    self.token2index = defaultdict(t2if)\n",
        "    self.index2token = defaultdict(i2tf)\n",
        "\n",
        "    if not map == None:\n",
        "      if type(map) is list:\n",
        "        for entry in map:\n",
        "          self.token2index[entry[0]] = entry[1]\n",
        "          self.index2token[entry[1]] = entry[0]\n",
        "      elif type(map) is set:\n",
        "        for index, token in enumerate(map):\n",
        "          self.token2index[token] = index\n",
        "          self.index2token[index] = token\n",
        "      else:\n",
        "        raise ValueError('map type must be a list of tuples or a set')\n",
        "\n",
        "\n",
        "  '''\n",
        "  to_indices method returns numeric ids corresponding to the aplhanumeric ids passed as parameter.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  tokens : list of alphanumeric ids\n",
        "  append : when set to true, if no numeric ids are found, new ids are assigned\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  List of numeric ids\n",
        "\n",
        "  '''\n",
        "  def to_indices(self, tokens : list, append : bool=True) -> list:\n",
        "    indices = []\n",
        "\n",
        "    for token in tokens:\n",
        "      if self.token2index[token] == -1 and append:\n",
        "        id = len(self.token2index) - 1\n",
        "        self.token2index[token] = id\n",
        "        self.index2token[id] = token\n",
        "\n",
        "      indices.append(self.token2index[token])\n",
        "\n",
        "    return indices\n",
        "\n",
        "  '''\n",
        "  to_tokens menthod returns alphanumeric ids corresponding to the numeri ids passed as parameter.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  indices : list of numeric ids\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  List of alphanumeric ids\n",
        "\n",
        "  '''\n",
        "  def to_tokens(self, indices : list) -> list:\n",
        "    return [self.index2token[index] for index in indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVSQeHFjj2Ew"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "UtilityMatrix class represents the relashoship between users and items.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "users_map       : bidirectional map of the users\n",
        "items_map       : bidirectional map of the items\n",
        "utility_matrix  : sparse matrix where each row correpsonds to a user and each column corresponds to a item\n",
        "\n",
        "'''\n",
        "class UtilityMatrix:\n",
        "\n",
        "  def __init__(self, users_map : BMap=None, items_map: BMap=None, utility_matrix : coo_array=None):\n",
        "    self.users_map = users_map\n",
        "    self.items_map = items_map\n",
        "    self.utility_matrix = utility_matrix\n",
        "    self.shape = utility_matrix.shape\n",
        "\n",
        "  def __getitem__(self, key):\n",
        "\n",
        "    if type(key) is str:\n",
        "      raise ValueError(\"Operation not supported yet, try with: um[['UID'], :], um[:, ['IID']], um['UID', 'IID']\")\n",
        "\n",
        "    if type(key[0]) is list and type(key[1]) is slice:\n",
        "\n",
        "      user_ids = [uid for uid in key[0] if type(uid) is str]\n",
        "      rows = self.users_map.to_indices(user_ids, append=False)\n",
        "\n",
        "      user_ids = [user_ids[i] for i in range(len(user_ids)) if rows[i] != -1]\n",
        "      rows = [row for row in rows if row != -1]\n",
        "\n",
        "      if len(rows) == 0:\n",
        "        raise ValueError('indeces out of boud')\n",
        "\n",
        "      coo_um = self.utility_matrix.tocsr()[rows, :].tocoo()\n",
        "\n",
        "      users_map = [(uid, i) for i, uid in enumerate(user_ids)]\n",
        "      items_map = [(iid, coo_um.col[i]) for i, iid in enumerate(self.items_map.to_tokens(coo_um.col))]\n",
        "\n",
        "      return UtilityMatrix(BMap(users_map), BMap(items_map), coo_um)\n",
        "\n",
        "\n",
        "    if type(key[0]) is slice and type(key[1]) is list:\n",
        "\n",
        "      item_ids = [iid for iid in key[1] if type(iid) is str]\n",
        "      columns = self.items_map.to_indices(item_ids, append=False)\n",
        "\n",
        "      item_ids = [item_ids[i] for i in range(len(item_ids)) if columns[i] != -1]\n",
        "      columns = [column for column in columns if column != -1]\n",
        "\n",
        "      if len(columns) == 0:\n",
        "        raise ValueError('indeces out of bound')\n",
        "\n",
        "      coo_um = self.utility_matrix.tocsc()[:, columns].tocoo()\n",
        "\n",
        "      items_map = [(iid, i) for i, iid in enumerate(item_ids)]\n",
        "      users_map = [(uid, coo_um.row[i])  for i, uid in enumerate(self.users_map.to_tokens(coo_um.row))]\n",
        "\n",
        "      return UtilityMatrix(BMap(users_map), BMap(items_map), coo_um)\n",
        "\n",
        "    if type(key[0]) is str and type(key[1]) is str:\n",
        "\n",
        "      row = self.users_map.to_indices([key[0]], append=False)[0]\n",
        "      column = self.items_map.to_indices([key[1]], append=False)[0]\n",
        "\n",
        "      if row == -1 or column == -1:\n",
        "        raise ValueError(\"index out of bound\")\n",
        "\n",
        "      return self.utility_matrix.tocsr()[[row], [column]][0]\n",
        "\n",
        "    raise ValueError(\"Operation not supported yet, try with: um[['UID'], :], um[:, ['IID']], um['UID', 'IID']\")\n",
        "\n",
        "  def __setitem__(self, key, value):\n",
        "\n",
        "    if type(key) is str:\n",
        "      raise ValueError(\"Operation not supported, try with um['UID', 'IID'] = V, um[['U1', 'U2', 'U3'], ['I1', 'I2', 'I3']] = [V1, V2, V3]\")\n",
        "\n",
        "    if type(key[0]) is str and type(key[1]) is str and (type(value) is float or type(value) is int):\n",
        "\n",
        "      row = self.users_map.to_indices([key[0]], append=False)[0]\n",
        "      column = self.items_map.to_indices([key[1]], append=False)[0]\n",
        "\n",
        "      if row == -1 or column == -1:\n",
        "        raise ValueError(\"index out of bound\")\n",
        "\n",
        "      lil_um = self.utility_matrix.tolil()\n",
        "\n",
        "      lil_um[[row], [column]] = value\n",
        "\n",
        "      self.utility_matrix = lil_um.tocoo()\n",
        "\n",
        "      return ;\n",
        "\n",
        "    if type(key[0]) is list and type(key[1]) is list and type(value) is list:\n",
        "\n",
        "      rows = []\n",
        "      columns = []\n",
        "      values = []\n",
        "\n",
        "      for entry in zip(key[0], key[1], value):\n",
        "\n",
        "        if not type(entry[0]) is str or not type(entry[1]) is str or (not type(entry[2]) is int and not type(entry[2]) is float):\n",
        "          continue\n",
        "\n",
        "        row = self.users_map.to_indices([entry[0]], append=False)[0]\n",
        "        column = self.items_map.to_indices([entry[1]], append=False)[0]\n",
        "\n",
        "        if row == -1 or column == -1:\n",
        "          continue\n",
        "\n",
        "        rows.append(row)\n",
        "        columns.append(column)\n",
        "        values.append(entry[2])\n",
        "\n",
        "      if len(rows) == 0:\n",
        "        raise ValueError('indeces out of bound')\n",
        "\n",
        "      lil_um = self.utility_matrix.tolil()\n",
        "\n",
        "      lil_um[rows, columns] = values\n",
        "\n",
        "      self.utility_matrix = lil_um.tocoo()\n",
        "\n",
        "      return ;\n",
        "\n",
        "    raise ValueError(\"Operation not supported, try with um['UID', 'IID'] = V, um[['U1', 'U2', 'U3'], ['I1', 'I2', 'I3']] = [V1, V2, V3]\")\n",
        "\n",
        "  '''\n",
        "  to_dense method converts the sparse matrix to a dense one\n",
        "\n",
        "  '''\n",
        "  def to_dense(self):\n",
        "    return self.utility_matrix.todense()\n",
        "\n",
        "  '''\n",
        "  get_active_reviewers method returns the top k active users\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  n : number of top k active users\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  List of the top k active user ids\n",
        "\n",
        "  '''\n",
        "  def get_active_reviewers(self, n: int):\n",
        "    csr_um = self.utility_matrix.tocsr()\n",
        "\n",
        "    index = np.zeros(shape=self.shape[0], dtype='int32')\n",
        "    nz_entries = np.zeros(shape=self.shape[0], dtype='int32')\n",
        "\n",
        "    for i, row in enumerate(np.unique(self.utility_matrix.row)):\n",
        "\n",
        "      start = csr_um.indptr[row]\n",
        "      end = csr_um.indptr[row + 1]\n",
        "      index[i] = row\n",
        "      nz_entries[i] = csr_um.indices[start : end].size\n",
        "\n",
        "    arg = nz_entries.argsort()[::-1]\n",
        "\n",
        "    return self.users_map.to_tokens((index[arg])[:n])\n",
        "\n",
        "  '''\n",
        "  train_test_mask method builds a random boolean mask used to hide elements during the training and testing phases.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train_size : precent of training samples for each row, 0.8 is the default value\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Random boolean mask as a dict where the key is the numeric id of the user\n",
        "\n",
        "  '''\n",
        "  def train_test_mask(self, train_size: float=0.8):\n",
        "    csr_um = self.utility_matrix.tocsr()\n",
        "    mask = dict()\n",
        "    rng = np.random.default_rng()\n",
        "\n",
        "    for row in np.unique(self.utility_matrix.row):\n",
        "      start = csr_um.indptr[row]\n",
        "      end = csr_um.indptr[row + 1]\n",
        "\n",
        "      cols = csr_um.indices[start : end]\n",
        "\n",
        "      n = round(cols.size * train_size)\n",
        "      msk = np.zeros(cols.size, dtype='bool')\n",
        "      msk[:n] = True\n",
        "      rng.shuffle(msk)\n",
        "      mask[row] = msk\n",
        "\n",
        "    return mask\n",
        "\n",
        "  def __str__(self):\n",
        "    coo_um_str = ''\n",
        "\n",
        "    if self.utility_matrix.row.shape[0] < 20:\n",
        "\n",
        "      user_ids = self.users_map.to_tokens(self.utility_matrix.row)\n",
        "      items_ids = self.items_map.to_tokens(self.utility_matrix.col)\n",
        "\n",
        "      for entry in zip(user_ids, items_ids, self.utility_matrix.data):\n",
        "        coo_um_str += \"  ({:<5}, {:>5}){:>6}\\n\".format(entry[0], entry[1], entry[2])\n",
        "\n",
        "      return f'{coo_um_str}\\n[{self.shape[0]} rows x {self.shape[1]} columns]'\n",
        "\n",
        "\n",
        "    user_ids = self.users_map.to_tokens(self.utility_matrix.row[:10])\n",
        "    items_ids = self.items_map.to_tokens(self.utility_matrix.col[:10])\n",
        "\n",
        "    for entry in zip(user_ids, items_ids, self.utility_matrix.data[:10]):\n",
        "      coo_um_str += \"  ({:<5}, {:>5}){:>6}\\n\".format(entry[0], entry[1], entry[2])\n",
        "\n",
        "    coo_um_str += '\\t . . . \\n'\n",
        "\n",
        "    user_ids = self.users_map.to_tokens(self.utility_matrix.row[-10:])\n",
        "    items_ids = self.items_map.to_tokens(self.utility_matrix.col[-10:])\n",
        "\n",
        "    for entry in zip(user_ids, items_ids, self.utility_matrix.data[-10:]):\n",
        "      coo_um_str += \"  ({:<5}, {:>5}){:>6}\\n\".format(entry[0], entry[1], entry[2])\n",
        "\n",
        "    return f'{coo_um_str}\\n[{self.shape[0]} rows x {self.shape[1]} columns]'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jR2AYIEokAhJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "load_reviews function reads the reviews in the file and stores them into a utility matrix.\n",
        "When multiple reviews made by the same user for the same item are found, the corresponding entry\n",
        "in the matrix is the average value.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "path      : dataset file path\n",
        "chunksize : size of the chunk\n",
        "\n",
        "Returns\n",
        "-------\n",
        "The utility matrix\n",
        "\n",
        "'''\n",
        "def load_reviews(path : str, chunksize: int) -> UtilityMatrix:\n",
        "\n",
        "  users_map = BMap()\n",
        "  items_map = BMap()\n",
        "\n",
        "  with pd.read_json(path, lines=True, chunksize=chunksize) as reader:\n",
        "\n",
        "    rows = []\n",
        "    columns = []\n",
        "    data = []\n",
        "    entries = defaultdict(lambda : -1)\n",
        "\n",
        "    for chunk in reader:\n",
        "      for entry in zip(chunk['user_id'], chunk['business_id'], chunk['stars']):\n",
        "\n",
        "        row = users_map.to_indices([entry[0]])[0]\n",
        "        column = items_map.to_indices([entry[1]])[0]\n",
        "\n",
        "        entry_idx = entries[(row, column)]\n",
        "\n",
        "        if entry_idx != -1:\n",
        "          data[entry_idx] = (data[entry_idx] + entry[2]) / 2\n",
        "        else:\n",
        "          rows.append(row)\n",
        "          columns.append(column)\n",
        "          data.append(entry[2])\n",
        "          entries[(row, column)] = len(entries) - 1\n",
        "\n",
        "\n",
        "  return UtilityMatrix(users_map, items_map, utility_matrix=coo_array((data, (rows, columns)), dtype='float32'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9drs0dx-kE-Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "load_business function reads the bussines info from file. Bussiness are sotred into a pandas data frame,\n",
        "each item feature selected is stored into a dictionary, unique features are collected into a set.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "path      : dataset file path\n",
        "chunksize : size of the chunck\n",
        "\n",
        "Returns\n",
        "-------\n",
        "A tuple containing:\n",
        "  - bussiness: dataframe\n",
        "  - items features: a dict where the key is the user id\n",
        "  - features set: the set of all possible features\n",
        "\n",
        "'''\n",
        "def load_business(path: str, chunksize: int) -> tuple:\n",
        "\n",
        "  item_features = defaultdict(list)\n",
        "  features_set = set()\n",
        "  business = pd.DataFrame()\n",
        "\n",
        "  with pd.read_json(path, lines=True, chunksize=chunksize) as reader:\n",
        "\n",
        "    for chunk in reader:\n",
        "\n",
        "      business = pd.concat([business, chunk[['business_id', 'name', 'address', 'city', 'state', 'postal_code']]])\n",
        "\n",
        "      for entry in zip(chunk['business_id'], chunk['city'], chunk['state'], chunk['is_open'], chunk['categories']):\n",
        "\n",
        "        item_features[entry[0]].append(entry[1])\n",
        "        item_features[entry[0]].append(entry[2])\n",
        "\n",
        "        features_set.add(entry[1])\n",
        "        features_set.add(entry[2])\n",
        "\n",
        "        if not entry[4] is None:\n",
        "          categories = entry[4].split(', ')\n",
        "          for cat in categories:\n",
        "            item_features[entry[0]].append(cat)\n",
        "            features_set.add(cat)\n",
        "        #else:\n",
        "        #   item_features[entry[0]]\n",
        "\n",
        "  return business, item_features, features_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XduUEHsx4hV"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "LSH class define the Locality Sensitive Hashing technique based on random hyperplanes.\n",
        "\n",
        "Parameters:\n",
        "-----------\n",
        "bands     : number of desidered bands\n",
        "sig_bits  : number of random hyperplanes\n",
        "ndim      : dimensions of datapoints\n",
        "'''\n",
        "class LSH:\n",
        "    def __init__(self, bands: int, sig_bits: int, ndim: int):\n",
        "        if sig_bits % bands != 0:\n",
        "            raise ValueError('The number of bands must be a divisor of sig_bits')\n",
        "\n",
        "        self.bands = bands\n",
        "        self.sig_bits = sig_bits\n",
        "        self.ndim = ndim\n",
        "        self.sig_hyperplanes = np.random.randn(sig_bits, ndim)\n",
        "        self.buckets = {i: defaultdict(set) for i in range(bands)}\n",
        "        self.powers = np.power(2, np.arange(sig_bits // bands))[::-1]\n",
        "\n",
        "    def _signature(self, hyperplanes: np.array, dtpoint: np.array):\n",
        "       return (np.dot(hyperplanes, dtpoint) > 0).astype('int8')\n",
        "\n",
        "    '''\n",
        "    hash method hashes the datapoint, then the numeric id is put into the bucket where the hashed band falls\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    _id     : numeric unique id of the datapoint\n",
        "    dtpoint : datapoint to be hashed\n",
        "    '''\n",
        "    def hash(self, _id: int, dtpoint: np.array):\n",
        "        dtp_sig = self._signature(self.sig_hyperplanes, dtpoint)\n",
        "        step = self.sig_bits // self.bands\n",
        "\n",
        "        for i in range(self.bands):\n",
        "            self.buckets[i][np.dot(self.powers, dtp_sig[i * step : (i + 1) * step])].add(_id)\n",
        "\n",
        "    '''\n",
        "    search method searches similar datpoints in the dataset. Searching is performed by using the same precedent approach\n",
        "    definded to hash datapoints, so, when a band falls into a bucket the numerics ids contained are returned. The hope\n",
        "    is that similar datapoints fall in the same buckets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    query : query to execute\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Similar datapoints\n",
        "    '''\n",
        "    def search(self, query) -> list:\n",
        "        query_sig = self._signature(self.sig_hyperplanes, query)\n",
        "        step = self.sig_bits // self.bands\n",
        "        query_rs = set()\n",
        "\n",
        "        for i in range(self.bands):\n",
        "            query_rs.update(self.buckets[i].get(np.dot(self.powers, query_sig[i * step : (i + 1) * step]), set()))\n",
        "\n",
        "        return query_rs\n",
        "\n",
        "    '''\n",
        "    merge method merges hashed datapoints from another LSH object with the same configuration\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    other : LSH object\n",
        "    '''\n",
        "    def merge(self, other):\n",
        "        for band, bucket in other.buckets.items():\n",
        "            for hash_val, itemset in bucket.items():\n",
        "                self.buckets[band][hash_val].update(itemset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MOwmlmcLsOt"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "ShardsHandler class is used to independently manage shard files where items and user profiles are written.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "path    : file path\n",
        "prefix  : perfix of the file name\n",
        "dtype   : file content's data type\n",
        "shape   : shape of the data (number of rows and columns)\n",
        "nbytes  : maximum number of bytes per shard\n",
        "\n",
        "'''\n",
        "class ShardsHandler:\n",
        "  def __init__(self, path: str, prefix: str, dtype: str, shape: tuple, nbytes: int):\n",
        "    self.path = path\n",
        "    self.prefix = prefix\n",
        "    self.dtype = np.dtype(dtype)\n",
        "    self.nbytes = nbytes\n",
        "    self.conf = (nbytes // (shape[1] * self.dtype.itemsize), shape[1])\n",
        "    self.shape = shape\n",
        "\n",
        "  def __setitem__(self, indices, values):\n",
        "\n",
        "    if isinstance(indices, int) or isinstance(indices, np.int64):\n",
        "      indices = np.array([indices])\n",
        "\n",
        "    if isinstance(indices, list):\n",
        "      indices = np.array(indices)\n",
        "\n",
        "    if not isinstance(indices, np.ndarray):\n",
        "      raise ValueError('indices must be a array like object')\n",
        "\n",
        "    if isinstance(values, int):\n",
        "      values = np.full(shape=(indices.size, self.conf[1]), fill_value=values, dtype=self.dtype)\n",
        "\n",
        "    if isinstance(values, list):\n",
        "      values = np.array(indices)\n",
        "\n",
        "    if not isinstance(values, np.ndarray):\n",
        "      raise ValueError('values must be a array like object')\n",
        "\n",
        "    asc_order = indices.argsort()\n",
        "    values = values[asc_order]\n",
        "    shard_ids = indices[asc_order] // self.conf[0]\n",
        "    local_indices = indices[asc_order] % self.conf[0]\n",
        "    shards, upper_bound = np.unique(shard_ids, return_index=True)\n",
        "\n",
        "    for shard_id, index, value in zip(shards, np.split(local_indices, upper_bound[1:]), np.split(values, upper_bound[1:])):\n",
        "      fname = f'{self.path}/{self.prefix}-{shard_id:03d}.dat'\n",
        "      if not os.path.exists(fname):\n",
        "        shard = np.memmap(fname, mode='w+', dtype=self.dtype, shape=self.conf)\n",
        "      else:\n",
        "        shard = np.memmap(fname, mode='r+', dtype=self.dtype, shape=self.conf)\n",
        "\n",
        "      shard[index] = value\n",
        "      shard.flush()\n",
        "\n",
        "    self.shape = (max(self.shape[0], indices.max()), self.shape[1])\n",
        "\n",
        "  def __getitem__(self, indices):\n",
        "\n",
        "    if isinstance(indices, int):\n",
        "      indices = np.array([indices])\n",
        "\n",
        "    if isinstance(indices, list):\n",
        "      indices = np.array(indices)\n",
        "\n",
        "    if not isinstance(indices, np.ndarray):\n",
        "      raise ValueError('indices must be a array like object')\n",
        "\n",
        "    asc_order = indices.argsort()\n",
        "    shard_ids = indices[asc_order] // self.conf[0]\n",
        "    local_indices = indices[asc_order] % self.conf[0]\n",
        "    shards, upper_bound = np.unique(shard_ids, return_index=True)\n",
        "\n",
        "    submat = []\n",
        "    for shard_id, index in zip(shards, np.split(local_indices, upper_bound[1:])):\n",
        "      shard = np.memmap(f'{self.path}/{self.prefix}-{shard_id:03d}.dat', mode='r+', dtype=self.dtype, shape=self.conf)\n",
        "      submat.append(shard[index])\n",
        "\n",
        "    if len(submat) == 1:\n",
        "      return submat[0]\n",
        "\n",
        "    return np.concatenate((submat), axis=0, dtype=self.dtype)[asc_order.argsort()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ug0zbDXH5Q4"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "OHEProcess class is a process lauched by the recommendation system object to compute one-hot-encoded items profiles\n",
        "'''\n",
        "class OHEProcess(multiprocessing.Process):\n",
        "  def __init__(self, oheh: ShardsHandler, shard_id: int, rps: int, item_ids: list, itemf: defaultdict, fmap: BMap):\n",
        "    super(OHEProcess, self).__init__()\n",
        "    self.fname = f'{oheh.path}/{oheh.prefix}-{shard_id:03d}.dat'\n",
        "    self.shard = np.memmap(self.fname, mode='w+', dtype=oheh.dtype, shape=oheh.conf)\n",
        "    self.shard_id = shard_id\n",
        "    self.shape = (rps, oheh.conf[1])\n",
        "    self.item_ids = item_ids\n",
        "    self.itemf = itemf\n",
        "    self.fmap = fmap\n",
        "\n",
        "  '''\n",
        "  run method defines the behavior of the process\n",
        "  '''\n",
        "  def run(self):\n",
        "    print(f'{self.name} encoding: shard_id: {self.shard_id}, shape: {self.shape}, dumping into {self.fname}')\n",
        "\n",
        "    exec_t = time.time()\n",
        "\n",
        "    for i, item_id in  enumerate(self.item_ids):\n",
        "      cols = self.fmap.to_indices(self.itemf[item_id], append=False)\n",
        "      self.shard[i, cols] = 1\n",
        "\n",
        "    self.shard.flush()\n",
        "    print(f'{self.name}: {self.fname} encoded in {round((time.time() - exec_t), 4)} (s)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a0NLfSegr5B"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "LSHProcess class is a process lauched by the recommendation system object to build the LSH index of items\n",
        "'''\n",
        "class LSHProcess(multiprocessing.Process):\n",
        "  def __init__(self, itemh: ShardsHandler, shard_id: int, bounds: tuple, lsh: LSH, queue: multiprocessing.Queue):\n",
        "    super(LSHProcess, self).__init__()\n",
        "    self.fname = f'{itemh.path}/{itemh.prefix}-{shard_id:03d}.dat'\n",
        "    self.batch = np.memmap(self.fname, mode='r+', dtype=itemh.dtype, shape=itemh.conf)\n",
        "    self.shard_id = shard_id\n",
        "    self.bounds = bounds\n",
        "    self.lsh = lsh\n",
        "    self.queue = queue\n",
        "\n",
        "  '''\n",
        "  run method defines the behavior of the process\n",
        "  '''\n",
        "  def run(self):\n",
        "    print(f'{self.name}: building LSH index: shard_id: {self.shard_id}, batch: {self.bounds}')\n",
        "\n",
        "    exec_t = time.time()\n",
        "\n",
        "    for i, row in enumerate(self.batch[self.bounds[0] : self.bounds[1]]):\n",
        "      self.lsh.hash(self.bounds[0] + i, row)\n",
        "\n",
        "    self.queue.put(self.lsh)\n",
        "    self.queue.close()\n",
        "\n",
        "    print(f'{self.name}: LSH index built in {round((time.time() - exec_t), 4)} (s)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDIjMoS7Jbil"
      },
      "outputs": [],
      "source": [
        "\n",
        "'''\n",
        "UserProcess class is a process lauched by the recommendation system object to compute user profiles\n",
        "'''\n",
        "class UserProcess(multiprocessing.Process):\n",
        "  def __init__(self, userh: ShardsHandler, itemh: ShardsHandler, shard_id: int, index: np.array, csr_um: csr_array, mask: dict):\n",
        "    super(UserProcess, self).__init__()\n",
        "    self.fname = f'{userh.path}/{userh.prefix}-{shard_id:03d}.dat'\n",
        "    self.shard = np.memmap(self.fname, mode='w+', dtype=userh.dtype, shape=userh.conf)\n",
        "    self.itemh = itemh\n",
        "    self.shard_id = shard_id\n",
        "    self.index = index\n",
        "    self.shape = (index.size, userh.conf[1])\n",
        "    self.csr_um = csr_um\n",
        "    self.mask = mask\n",
        "\n",
        "  '''\n",
        "  run method defines the behavior of the process\n",
        "  '''\n",
        "  def run(self):\n",
        "    print(f'{self.name} encoding: shard_id: {self.shard_id}, shape: {self.shape}, dumping into {self.fname}')\n",
        "\n",
        "    exec_t = time.time()\n",
        "\n",
        "    for i, user_indx in enumerate(self.index):\n",
        "      start = self.csr_um.indptr[user_indx]\n",
        "      end = self.csr_um.indptr[user_indx + 1]\n",
        "\n",
        "      cols = self.csr_um.indices[start : end]\n",
        "      ratings = self.csr_um.data[start : end]\n",
        "\n",
        "      cols = cols[self.mask[user_indx]]\n",
        "      ratings = ratings[self.mask[user_indx]]\n",
        "\n",
        "      item_profiles = self.itemh[cols]\n",
        "\n",
        "\n",
        "      userp = np.dot(ratings - ratings.mean(), item_profiles)\n",
        "      #userp = np.dot(ratings, item_profiles) / ratings.sum()\n",
        "      #userp = np.average(item_profiles, axis=0, weights=(ratings - (ratings.mean() - 1e-6)))\n",
        "      #userp = userp / np.linalg.norm(userp)\n",
        "\n",
        "      self.shard[i] = userp\n",
        "\n",
        "    self.shard.flush()\n",
        "    print(f'{self.name}: {self.fname} encoded in {round((time.time() - exec_t), 4)} (s)')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xz9bY2v7kMNr"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "RecommendationSystem class reprenset the content-based recommendation system\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "utility_matrix : utility matrix containing the relashponship between users and items\n",
        "items          : dataframe containing basic info about items\n",
        "item_features  : dictionary containing features of each items (the key is the items id)\n",
        "features_set   : set of all possible features\n",
        "bpsh           : maximum number of bytes per shard\n",
        "ncomps         : dimensionality of reduced items\n",
        "\n",
        "'''\n",
        "class RecommendationSystem:\n",
        "  def __init__(self, utility_matrix: UtilityMatrix, items: pd.DataFrame, item_features: defaultdict, features_set : set, bpsh: int, ncomps: int):\n",
        "    self.utility_matrix = utility_matrix\n",
        "    self.items = items\n",
        "    self.item_features = item_features\n",
        "    self.features_map = BMap(features_set)\n",
        "    self.checkpoint = f'checkpoint-{round(time.time())}'\n",
        "    self.shards_dir = f'{self.checkpoint}/shards'\n",
        "    self.ohe_handler = ShardsHandler(path=self.shards_dir, prefix='ohe', dtype='int8', shape=(len(item_features), len(features_set)), nbytes=bpsh)\n",
        "    self.items_handler = ShardsHandler(path=self.shards_dir, prefix='itemp', dtype='float32', shape=(len(item_features), ncomps), nbytes=bpsh)\n",
        "    self.users_handler = ShardsHandler(path=self.shards_dir, prefix='userp', dtype='float32', shape=(utility_matrix.shape[0], ncomps), nbytes=bpsh)\n",
        "    self.lsh = LSH(bands=30, sig_bits=180, ndim=ncomps)\n",
        "\n",
        "    if not os.path.exists(self.checkpoint):\n",
        "      os.mkdir(self.checkpoint)\n",
        "      os.mkdir(self.shards_dir)\n",
        "\n",
        "  def _reduce_dimensionality(self):\n",
        "\n",
        "    pname = multiprocessing.current_process().name\n",
        "    print(f'{pname}: reducing dimensionality: (n, {self.ohe_handler.shape[1]}) -> (n, {self.items_handler.shape[1]})')\n",
        "\n",
        "    rps = self.ohe_handler.conf[0]\n",
        "    n_shard = self.ohe_handler.shape[0] // rps + 1\n",
        "\n",
        "    exec_t = time.time()\n",
        "\n",
        "    optimal_components = []\n",
        "    for i in range(1, n_shard + 1):\n",
        "      if i * rps > self.ohe_handler.shape[0]: rps = rps - (i  * rps - self.ohe_handler.shape[0])\n",
        "\n",
        "      index = (i - 1) * self.ohe_handler.conf[0] + np.arange(0, rps)\n",
        "      shard = self.ohe_handler[index]\n",
        "\n",
        "      shard_gpu = cp.asarray(shard, dtype='float32')\n",
        "\n",
        "      cov = cp.cov(shard_gpu, rowvar=False)\n",
        "      eigenvalues, eigenvectors = cp.linalg.eigh(cov)\n",
        "      sorted_indices = cp.argsort(eigenvalues)[::-1]\n",
        "      eigenvectors = eigenvectors[:, sorted_indices]\n",
        "      eigenvalues = eigenvalues[sorted_indices]\n",
        "\n",
        "      expl_var = eigenvalues / cp.sum(eigenvalues)\n",
        "      cum_expl_var = cp.asnumpy(cp.cumsum(expl_var))\n",
        "\n",
        "      optimal_components.append(np.argwhere(cum_expl_var >= 0.8)[0, 0])\n",
        "\n",
        "      shard = cp.asnumpy(cp.dot(shard_gpu, eigenvectors[:, : self.items_handler.shape[1]])).astype(self.items_handler.dtype)\n",
        "\n",
        "      self.items_handler[index] = shard\n",
        "\n",
        "\n",
        "    print(f'Number of optimal components with cumulative explained variance greater than 0.8: {round(np.mean(optimal_components))}')\n",
        "    print(f'{pname}: dimensionality reduced in {round((time.time() - exec_t), 4)} (s)')\n",
        "\n",
        "  def _encode_items(self):\n",
        "\n",
        "    items_map = self.utility_matrix.items_map\n",
        "    rps = self.ohe_handler.conf[0]\n",
        "    n_shard = self.ohe_handler.shape[0] // rps + 1\n",
        "\n",
        "    processes = []\n",
        "    for i in range(1, n_shard + 1):\n",
        "      if i * rps > self.ohe_handler.shape[0]: rps = rps - (i  * rps - self.ohe_handler.shape[0])\n",
        "\n",
        "      item_ids = items_map.to_tokens((i - 1) * self.ohe_handler.conf[0] + np.arange(0, rps))\n",
        "\n",
        "      p = OHEProcess(self.ohe_handler, (i - 1), rps, item_ids, self.item_features, self.features_map)\n",
        "      p.start()\n",
        "\n",
        "      processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "      p.join()\n",
        "\n",
        "    self._reduce_dimensionality()\n",
        "\n",
        "    queue = multiprocessing.Queue()\n",
        "    ncpu = os.cpu_count()\n",
        "    rps = self.items_handler.conf[0]\n",
        "    n_shard = self.items_handler.shape[0] // rps + 1\n",
        "\n",
        "    for i in range(1, n_shard + 1):\n",
        "      if i * rps > self.items_handler.shape[0]: rps = rps - (i  * rps - self.items_handler.shape[0])\n",
        "\n",
        "      batch_size = rps // ncpu + 1\n",
        "\n",
        "      for icpu in range(1, ncpu + 1):\n",
        "        batch_shape = ((icpu - 1) * batch_size, icpu * batch_size)\n",
        "\n",
        "        p = LSHProcess(self.items_handler, (i - 1), batch_shape, self.lsh, queue)\n",
        "        p.start()\n",
        "\n",
        "    for _ in range(n_shard * ncpu):\n",
        "      self.lsh.merge(queue.get())\n",
        "\n",
        "  def _encode_users(self):\n",
        "\n",
        "    rps = self.users_handler.conf[0]\n",
        "    n_shard = self.users_handler.shape[0] // rps + 1\n",
        "\n",
        "    csr_um = self.utility_matrix.utility_matrix.tocsr()\n",
        "\n",
        "    processes = []\n",
        "    for i in range(1, n_shard + 1):\n",
        "      if i * rps > self.users_handler.shape[0]: rps = rps - (i  * rps - self.users_handler.shape[0])\n",
        "\n",
        "      index = (i - 1) * self.users_handler.conf[0] + np.arange(0, rps)\n",
        "\n",
        "      p = UserProcess(self.users_handler, self.items_handler, (i - 1), index, csr_um, self.mask)\n",
        "      p.start()\n",
        "\n",
        "      processes.append(p)\n",
        "\n",
        "    for p in processes:\n",
        "      p.join()\n",
        "\n",
        "  '''\n",
        "  transform method computes item profiles and user profiles. Items are encoded in one-hot representation, and then,\n",
        "  their dimensionality is reduced using PCA technique. User profiles are computed from item profiles\n",
        "  rated by the user. Before encoding user profiles, a random boolean mask is computed, which is useful for masking items\n",
        "  during the training and evaluation phase.\n",
        "  '''\n",
        "  def transform(self):\n",
        "\n",
        "    self._encode_items()\n",
        "    self.mask = self.utility_matrix.train_test_mask(0.7)\n",
        "    self._encode_users()\n",
        "\n",
        "  def _cosine_similarity(self, user_profile: np.array, item_profiles: np.array) -> np.array:\n",
        "    return np.dot(item_profiles, user_profile) / (np.linalg.norm(user_profile) * np.linalg.norm(item_profiles, axis=1))\n",
        "\n",
        "  '''\n",
        "  recommend method recommends to a user a list of interesting items, based on his/her preferences\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  user_ids : list of users ids\n",
        "  n        : maximum number of recommended items\n",
        "\n",
        "  '''\n",
        "  def recommend(self, user_ids: list, n=10) -> dict:\n",
        "\n",
        "    result_set = dict()\n",
        "    csr_um = self.utility_matrix.utility_matrix.tocsr()\n",
        "    users_map = self.utility_matrix.users_map\n",
        "\n",
        "    for user_id in user_ids:\n",
        "\n",
        "      user_indx = users_map.to_indices([user_id], append=False)[0]\n",
        "      user_profile = self.users_handler[user_indx][0]\n",
        "\n",
        "      start = csr_um.indptr[user_indx]\n",
        "      end = csr_um.indptr[user_indx + 1]\n",
        "\n",
        "      items_rated = set(csr_um.indices[start : end])\n",
        "\n",
        "      items_indx = self.lsh.search(user_profile)\n",
        "      items_indx = set.difference(items_indx, items_rated)\n",
        "\n",
        "      itemps = self.items_handler[list(items_indx)]\n",
        "\n",
        "      scores = self._cosine_similarity(user_profile, itemps)\n",
        "\n",
        "      argscores = scores.argsort()[::-1][:n]\n",
        "\n",
        "      scores = (scores[argscores] + 1) / 2\n",
        "      top_items_indx = np.array(list(items_indx))[argscores]\n",
        "      top_items = self.items.iloc[top_items_indx, :][['name', 'address', 'city', 'state', 'postal_code']]\n",
        "\n",
        "      result_set[user_id] = [('; '.join(row[1]), round(scores[i], 8)) for i, row in enumerate(top_items.iterrows())]\n",
        "\n",
        "    return result_set\n",
        "\n",
        "  def _precision_recall_at_k(self, csr_um, user_indx, user_profile, items_rated, ratings, step):\n",
        "\n",
        "    mean = ratings[self.mask[user_indx]].mean()\n",
        "\n",
        "    items_rated = items_rated[~self.mask[user_indx]]\n",
        "    ratings = ratings[~self.mask[user_indx]]\n",
        "\n",
        "    items_indx = self.lsh.search(user_profile)\n",
        "\n",
        "    common_items = set.intersection(set(items_rated), items_indx)\n",
        "    common_items_ratings = csr_um[[user_indx], list(common_items)]\n",
        "    ground_truth = (common_items_ratings -  mean > 0)\n",
        "\n",
        "    itemps = self.items_handler[list(common_items)]\n",
        "\n",
        "    scores = self._cosine_similarity(user_profile, itemps)\n",
        "    desc_order = scores.argsort()[::-1]\n",
        "\n",
        "    k_range = np.arange(step, len(common_items) , step)\n",
        "    precision = []\n",
        "    recall = []\n",
        "    for k in k_range:\n",
        "      rel_items = ((scores[desc_order[:k]] > 0) & ground_truth[desc_order[:k]]).sum()\n",
        "      tot_rel_items = ground_truth[desc_order[:k]].sum()\n",
        "      precision.append(rel_items / k)\n",
        "      recall.append(rel_items / tot_rel_items)\n",
        "\n",
        "    return k_range, precision, recall\n",
        "\n",
        "  def _diversity(self, recomended_items):\n",
        "    jd_mat = np.zeros(shape=(recomended_items.shape[0], recomended_items.shape[0]), dtype='float16')\n",
        "\n",
        "    for i in range(recomended_items.shape[0]):\n",
        "      for j in range(i, recomended_items.shape[0]):\n",
        "        jd = 1 - (recomended_items[i] & recomended_items[j]).sum() / (recomended_items[i] | recomended_items[j]).sum()\n",
        "        jd_mat[i, j] = jd\n",
        "        jd_mat[j, i] = jd\n",
        "\n",
        "    return np.mean(jd_mat.sum(axis=1) / (recomended_items.shape[0] - 1))\n",
        "\n",
        "  def _diversity_at_k(self, user_profile, items_rated, step, top_n):\n",
        "\n",
        "    items_indx = self.lsh.search(user_profile)\n",
        "    items_indx = set.difference(items_indx, set(items_rated))\n",
        "\n",
        "    item_profiles = self.items_handler[list(items_indx)]\n",
        "\n",
        "    scores = self._cosine_similarity(user_profile, item_profiles)\n",
        "    argscores = scores.argsort()[::-1]\n",
        "\n",
        "    ohe_profiles = self.ohe_handler[list(items_indx)]\n",
        "    ohe_profiles = ohe_profiles[argscores][:top_n]\n",
        "\n",
        "    k_range = np.arange(step, ohe_profiles.shape[0] , step)\n",
        "    diversity = []\n",
        "    for k in k_range:\n",
        "      diversity.append(self._diversity(ohe_profiles[:k]))\n",
        "\n",
        "    return k_range, diversity\n",
        "\n",
        "  '''\n",
        "  evaluate_query method tries to evaluate a query computing the Precision@K, Recall@K and Diversity@K\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  user_id : alphanumeric user id\n",
        "  step    : step size for metric computation\n",
        "  n       : maximum number of test items\n",
        "  '''\n",
        "  def evaluate_query(self, user_id: str, step: int=5, n: int=200):\n",
        "\n",
        "    csr_um = self.utility_matrix.utility_matrix.tocsr()\n",
        "    users_map = self.utility_matrix.users_map\n",
        "\n",
        "    user_indx = users_map.to_indices([user_id], append=False)[0]\n",
        "    user_profile = self.users_handler[user_indx][0]\n",
        "\n",
        "    start = csr_um.indptr[user_indx]\n",
        "    end = csr_um.indptr[user_indx + 1]\n",
        "\n",
        "    items_rated = csr_um.indices[start : end]\n",
        "    ratings = csr_um.data[start : end]\n",
        "\n",
        "    k_range, precision, recall = self._precision_recall_at_k(csr_um, user_indx, user_profile, items_rated, ratings, step)\n",
        "\n",
        "    k_range1, diveristy = self._diversity_at_k(user_profile, items_rated, step, n)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
        "    fig.suptitle(f'Query: {user_id}', fontsize=12)\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    ax1.set_title('Precision & Recall @ K', fontsize=10)\n",
        "    ax1.plot(k_range, precision, label='Precision')\n",
        "    ax1.plot(k_range, recall, label='Recall')\n",
        "    ax1.legend(loc='upper right')\n",
        "    ax1.set_ylim(0, 1.1)\n",
        "    ax1.set_xlabel('K')\n",
        "    ax1.set_ylabel('Precision & Recall')\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.set_title('Diversity @ K', fontsize=10)\n",
        "    ax2.plot(k_range1, diveristy, label='Diveristy')\n",
        "    ax2.legend(loc='upper right')\n",
        "    ax2.set_ylim(0, 1.1)\n",
        "    ax2.set_xlabel('K')\n",
        "    ax2.set_ylabel('Diversity')\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "  '''\n",
        "  evaluate_system method tries to evaluate the overall performance of the recommendation system\n",
        "  computing the MAP@K metric\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  user_ids : alphanumeric user ids\n",
        "  step     : step size for metric computation\n",
        "  n        : maximum number of test items\n",
        "  '''\n",
        "  def evaluate_system(self, user_ids, step: int=10, n: int=180):\n",
        "    csr_um = self.utility_matrix.utility_matrix.tocsr()\n",
        "    users_map = self.utility_matrix.users_map\n",
        "    m_range = np.arange(step, n, step)\n",
        "\n",
        "    apk = []\n",
        "    for user_id in user_ids:\n",
        "      user_indx = users_map.to_indices([user_id], append=False)[0]\n",
        "      user_profile = self.users_handler[user_indx][0]\n",
        "\n",
        "      start = csr_um.indptr[user_indx]\n",
        "      end = csr_um.indptr[user_indx + 1]\n",
        "\n",
        "      items_rated = csr_um.indices[start : end]\n",
        "      ratings = csr_um.data[start : end]\n",
        "\n",
        "      items_rated = items_rated[~self.mask[user_indx]]\n",
        "\n",
        "      items_indx = self.lsh.search(user_profile)\n",
        "\n",
        "      test_items = set.intersection(set(items_rated), items_indx)\n",
        "      test_ratings = csr_um[[user_indx], list(test_items)]\n",
        "      ground_truth = (test_ratings -  ratings[self.mask[user_indx]].mean() > 0)\n",
        "\n",
        "      itemps = self.items_handler[list(test_items)]\n",
        "\n",
        "      scores = self._cosine_similarity(user_profile, itemps)\n",
        "      desc_order = scores.argsort()[::-1]\n",
        "\n",
        "      scores = scores[desc_order]\n",
        "      ground_truth = ground_truth[desc_order]\n",
        "      ind_fnct = ground_truth.astype('int8')\n",
        "\n",
        "      apk.append([1 / ind_fnct[:m].sum() * sum([((scores[:k] > 0) & ground_truth[:k]).sum() / k * ind_fnct[k] for k in range(1, scores[:m].size)]) for m in m_range])\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6.3, 2))\n",
        "    fig.suptitle(f'Mean Average Precision @ K', fontsize=12)\n",
        "\n",
        "    ax.plot(m_range, np.mean(apk, axis=0), label='MAP@K')\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set_xlabel('K')\n",
        "    ax.set_ylabel('MAP')\n",
        "    ax.grid(True)\n",
        "\n",
        "    plt.show()\n",
        "    fig.savefig('map.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2LpAl5_pRBq",
        "outputId": "d699ec7f-bc08-4e67-d862-9aa2ec06a69f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building utility matrix...\n"
          ]
        }
      ],
      "source": [
        "print('Building utility matrix...')\n",
        "um = load_reviews(path='yelp_academic_dataset_review.json.zip', chunksize=10e4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN_sfyqwpNqv",
        "outputId": "6aa89049-ad61-4763-b223-9843ef899777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading business...\n"
          ]
        }
      ],
      "source": [
        "print('Loading business...')\n",
        "bus, itemfs, fset = load_business(path='yelp_academic_dataset_business.json.zip', chunksize=10e4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsKgQ48JtUBm"
      },
      "outputs": [],
      "source": [
        "print('Transforming...')\n",
        "rs = RecommendationSystem(utility_matrix=um, items=bus, item_features=itemfs, features_set=fset, bpsh=100 * 2 ** 20, ncomps=210)\n",
        "rs.transform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6hZS_F6VuWnN"
      },
      "outputs": [],
      "source": [
        "top_k = um.get_active_reviewers(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59yINsY9pqhf"
      },
      "outputs": [],
      "source": [
        "rs.evaluate_query(top_k[0], 5, 180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpq-ZMsznXzC"
      },
      "outputs": [],
      "source": [
        "rs.evaluate_system(top_k[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCDeL3Zs8_3D"
      },
      "outputs": [],
      "source": [
        "p = rs.recommend(top_k[:10], 100)\n",
        "\n",
        "for q in p:\n",
        "  print(q, p[q])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}